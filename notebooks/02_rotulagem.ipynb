{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição das empresas por estrato inicial:\n",
      "strata\n",
      "M4 Transformacao Digital    464\n",
      "M1 Agro                     262\n",
      "Nao_Classificado            215\n",
      "M2 Saude                    203\n",
      "M3 Infra Mobilidade         203\n",
      "M5 Bioeconomia Energia       22\n",
      "M6 Defesa Soberania          16\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "Tamanho da amostra final: 302 empresas.\n",
      "\n",
      "Distribuição na amostra (deve ser proporcional à original se estratificada):\n",
      "strata\n",
      "M4 Transformacao Digital    101\n",
      "M1 Agro                      57\n",
      "Nao_Classificado             47\n",
      "M2 Saude                     44\n",
      "M3 Infra Mobilidade          44\n",
      "M5 Bioeconomia Energia        5\n",
      "M6 Defesa Soberania           4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Arquivo com as amostras a serem rotuladas foi salvo em: '../data/processed/amostra_para_rotulagem.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# --- Carregue o resultado do seu script de classificação ---\n",
    "classified_file = '../reports/classificacao_missoes_keywords_v4.json'\n",
    "try:\n",
    "    df = pd.read_json(classified_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: Arquivo de classificação '{classified_file}' não encontrado.\")\n",
    "    print(\"Por favor, execute o script 'src/classify_by_keywords.py' primeiro.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Preparação do Estrato ---\n",
    "# Vamos criar uma coluna de estrato para simplificar.\n",
    "# Usaremos a primeira missão na lista de 'missoes_atribuidas'.\n",
    "# Se estiver vazio, marcaremos como 'Nao_Classificado'.\n",
    "df['strata'] = df['missoes_atribuidas'].apply(lambda x: x.split(', ')[0] if x else 'Nao_Classificado')\n",
    "\n",
    "# Verifique a distribuição para garantir que temos estratos viáveis\n",
    "print(\"Distribuição das empresas por estrato inicial:\")\n",
    "print(df['strata'].value_counts())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- Cálculo do Tamanho da Amostra ---\n",
    "population_size = len(df)\n",
    "# Usando o número recomendado para 95% de confiança e 5% de margem de erro\n",
    "sample_size = 302 \n",
    "\n",
    "if population_size < sample_size:\n",
    "    sample_size = population_size # Garante que não tentemos amostrar mais do que a população\n",
    "    print(f\"AVISO: O tamanho da população ({population_size}) é menor que o tamanho de amostra desejado. Usando todas as empresas.\")\n",
    "\n",
    "\n",
    "# --- Amostragem Estratificada ---\n",
    "# Precisamos de um DataFrame com apenas os índices e o estrato\n",
    "df_for_sampling = df[['strata']].copy()\n",
    "\n",
    "# A função train_test_split é excelente para amostragem estratificada.\n",
    "# Vamos usá-la para \"dividir\" o dataset, mas apenas pegaremos a amostra 'test_set'.\n",
    "# O 'train_set' será o restante das empresas não selecionadas.\n",
    "try:\n",
    "    _, sample_df = train_test_split(\n",
    "        df_for_sampling,\n",
    "        test_size=sample_size,\n",
    "        stratify=df_for_sampling['strata'],\n",
    "        random_state=42 # Para reprodutibilidade\n",
    "    )\n",
    "    # Obtenha o DataFrame completo apenas com as amostras selecionadas\n",
    "    final_sample_to_label = df.loc[sample_df.index]\n",
    "except ValueError as e:\n",
    "    print(f\"AVISO: Não foi possível realizar a amostragem estratificada. Provavelmente alguns estratos têm poucos membros. {e}\")\n",
    "    print(\"Realizando amostragem aleatória simples como alternativa.\")\n",
    "    final_sample_to_label = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "\n",
    "# --- Verificação Final e Salvamento ---\n",
    "print(f\"Tamanho da amostra final: {len(final_sample_to_label)} empresas.\")\n",
    "print(\"\\nDistribuição na amostra (deve ser proporcional à original se estratificada):\")\n",
    "print(final_sample_to_label['strata'].value_counts())\n",
    "\n",
    "# Garante que o diretório de destino exista\n",
    "output_dir = os.path.join('../data', 'processed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Salve a lista de empresas a serem rotuladas em um arquivo CSV\n",
    "output_path = os.path.join(output_dir, 'amostra_para_rotulagem.csv')\n",
    "final_sample_to_label.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\nArquivo com as amostras a serem rotuladas foi salvo em: '{output_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparar pro doccano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando amostra do arquivo: ../data/processed/amostra_para_rotulagem.csv\n",
      "Iniciando conversão para o formato JSONL...\n",
      "--------------------------------------------------\n",
      "Conversão concluída com sucesso!\n",
      "Arquivo de saída pronto para importação no Doccano: ../data/processed/amostra_para_doccano.jsonl\n",
      "No Doccano, importe este arquivo usando o formato 'JSONL'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- CONFIGURAÇÃO ---\n",
    "INPUT_FILE = os.path.join('../data', 'processed', 'amostra_para_rotulagem.csv')\n",
    "OUTPUT_FILE = os.path.join('../data', 'processed', 'amostra_para_doccano.jsonl')\n",
    "\n",
    "# Coluna principal que será o texto a ser rotulado\n",
    "TEXT_COLUMN = 'descricao_organizacao'\n",
    "\n",
    "# Colunas que você quer visualizar como contexto adicional (metadados)\n",
    "METADATA_COLUMNS = [\n",
    "    'nome_organizacao',\n",
    "    'segmento_atuacao',\n",
    "    'tecnologias_disruptivas',\n",
    "]\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Converte o arquivo CSV de amostra para o formato JSONL que o Doccano\n",
    "    utiliza para importar texto com metadados.\n",
    "    \"\"\"\n",
    "    print(f\"Carregando amostra do arquivo: {INPUT_FILE}\")\n",
    "    \n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"ERRO: Arquivo de entrada não encontrado em '{INPUT_FILE}'.\")\n",
    "        print(\"Por favor, execute o script de seleção de amostra primeiro.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    print(\"Iniciando conversão para o formato JSONL...\")\n",
    "    \n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        for index, row in df.iterrows():\n",
    "            # Cria um dicionário para os metadados\n",
    "            metadata = {col: row[col] for col in METADATA_COLUMNS if col in row and pd.notna(row[col])}\n",
    "            \n",
    "            # Monta o objeto JSON para a linha atual\n",
    "            # A chave 'text' é padrão do Doccano para o texto principal\n",
    "            # A chave 'meta' é padrão do Doccano para os metadados\n",
    "            json_record = {\n",
    "                'text': row[TEXT_COLUMN],\n",
    "                'meta': metadata \n",
    "            }\n",
    "            \n",
    "            # Escreve o registro JSON como uma nova linha no arquivo\n",
    "            f.write(json.dumps(json_record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Conversão concluída com sucesso!\")\n",
    "    print(f\"Arquivo de saída pronto para importação no Doccano: {OUTPUT_FILE}\")\n",
    "    print(\"No Doccano, importe este arquivo usando o formato 'JSONL'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a busca por novas amostras para rotulagem (excluindo M6)...\n",
      "1385 empresas no total, 300 já rotuladas.\n",
      "Restam 1087 empresas não rotuladas para amostragem.\n",
      "Encontrados 419 candidatos para as missões M1_Agro, M2_Saude, M5_Bioeconomia_Energia.\n",
      "--------------------------------------------------\n",
      "Amostragem concluída.\n",
      "Tamanho da nova amostra para rotulagem: 75 empresas.\n",
      "\n",
      "Distribuição preliminar da nova amostra (baseada no modelo de keywords):\n",
      "M1_Agro                   34\n",
      "M2_Saude                  36\n",
      "M5_Bioeconomia_Energia    18\n",
      "dtype: int64\n",
      "\n",
      "Arquivo com as novas amostras a serem rotuladas foi salvo em: '../data/processed/amostra_para_rotulagem_batch_2.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURAÇÃO ---\n",
    "# Arquivo com a classificação preliminar de TODAS as empresas\n",
    "CLASSIFIED_FULL_DATASET_PATH = os.path.join('../reports', 'classificacao_missoes_keywords_v4.json')\n",
    "# Arquivo com as empresas que você JÁ ROTULOU\n",
    "LABELED_DATA_PATH = os.path.join('../data', 'processed', 'rotulados.csv')\n",
    "# Arquivo de saída para a nova leva de rotulagem\n",
    "OUTPUT_PATH = os.path.join('../data', 'processed', 'amostra_para_rotulagem_batch_2.csv')\n",
    "\n",
    "# Missões que queremos fortalecer (nosso alvo). M6 foi removida para tratamento manual.\n",
    "TARGET_MISSIONS = ['M1_Agro', 'M2_Saude', 'M5_Bioeconomia_Energia']\n",
    "# Número de novas amostras que queremos coletar\n",
    "N_NEW_SAMPLES = 75\n",
    "\n",
    "def get_new_samples():\n",
    "    \"\"\"\n",
    "    Identifica e seleciona novas amostras para rotulagem, focando em missões\n",
    "    específicas e excluindo empresas já rotuladas.\n",
    "    \"\"\"\n",
    "    print(\"Iniciando a busca por novas amostras para rotulagem (excluindo M6)...\")\n",
    "\n",
    "    # 1. Carregar os datasets necessários\n",
    "    try:\n",
    "        df_full = pd.read_json(CLASSIFIED_FULL_DATASET_PATH)\n",
    "        df_labeled = pd.read_csv(LABELED_DATA_PATH)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERRO: Não foi possível encontrar um dos arquivos necessários: {e}\")\n",
    "        print(\"Verifique se os caminhos CLASSIFIED_FULL_DATASET_PATH e LABELED_DATA_PATH estão corretos.\")\n",
    "        return\n",
    "\n",
    "    print(f\"{len(df_full)} empresas no total, {len(df_labeled)} já rotuladas.\")\n",
    "\n",
    "    # 2. Remover empresas que já foram rotuladas\n",
    "    labeled_names = df_labeled['Organização'].unique()\n",
    "    df_unlabeled = df_full[~df_full['nome_organizacao'].isin(labeled_names)].copy()\n",
    "    print(f\"Restam {len(df_unlabeled)} empresas não rotuladas para amostragem.\")\n",
    "\n",
    "    # 3. Filtrar para encontrar os melhores candidatos\n",
    "    # Criamos uma máscara booleana: um candidato é válido se a sua classificação\n",
    "    # preliminar (por keywords) for em QUALQUER uma das nossas missões alvo.\n",
    "    candidate_mask = df_unlabeled[TARGET_MISSIONS].sum(axis=1) > 0\n",
    "    df_candidates = df_unlabeled[candidate_mask]\n",
    "    \n",
    "    if len(df_candidates) == 0:\n",
    "        print(\"Nenhum novo candidato encontrado para as missões alvo. Saindo.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Encontrados {len(df_candidates)} candidatos para as missões {', '.join(TARGET_MISSIONS)}.\")\n",
    "\n",
    "    # 4. Amostrar a partir dos candidatos\n",
    "    # Como não estamos mais priorizando a M6, fazemos uma amostragem aleatória simples\n",
    "    # do nosso pool de candidatos qualificados.\n",
    "    if len(df_candidates) < N_NEW_SAMPLES:\n",
    "        print(f\"AVISO: O número de candidatos ({len(df_candidates)}) é menor que o tamanho da amostra desejado ({N_NEW_SAMPLES}). Usando todos os candidatos.\")\n",
    "        sample_size = len(df_candidates)\n",
    "    else:\n",
    "        sample_size = N_NEW_SAMPLES\n",
    "        \n",
    "    final_sample_to_label = df_candidates.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"Amostragem concluída.\")\n",
    "    print(f\"Tamanho da nova amostra para rotulagem: {len(final_sample_to_label)} empresas.\")\n",
    "    print(\"\\nDistribuição preliminar da nova amostra (baseada no modelo de keywords):\")\n",
    "    # Mostra a contagem para as missões que estávamos buscando\n",
    "    print(final_sample_to_label[TARGET_MISSIONS].sum())\n",
    "\n",
    "    # 5. Salvar o resultado\n",
    "    # Selecionamos apenas as colunas originais para manter o formato limpo\n",
    "    output_cols = ['nome_organizacao', 'descricao_organizacao', 'categoria_organizacao', 'segmento_atuacao', 'tecnologias_disruptivas', 'fase_negocio', 'papel_no_ecossistema']\n",
    "    \n",
    "    # Garante que todas as colunas existem no dataframe antes de salvar\n",
    "    final_output_cols = [col for col in output_cols if col in final_sample_to_label.columns]\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "    final_sample_to_label[final_output_cols].to_csv(OUTPUT_PATH, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\nArquivo com as novas amostras a serem rotuladas foi salvo em: '{OUTPUT_PATH}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_new_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a integração dos novos rótulos...\n",
      "Golden Dataset original com 310 registros.\n",
      "Novo batch com 75 novos rótulos.\n",
      "O novo Golden Dataset combinado tem 374 registros.\n",
      "--------------------------------------------------\n",
      "Integração concluída com sucesso!\n",
      "Novo Golden Dataset salvo em: '../data/processed/golden_dataset_v2.csv'\n",
      "\n",
      "Próximo passo recomendado: Execute novamente o script 'train_evaluate_model_v4.py',\n",
      "ajustando o caminho para carregar este novo arquivo.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURAÇÃO DE ARQUIVOS ---\n",
    "# Dataset original com todos os dados rotulados\n",
    "ORIGINAL_GOLDEN_DATASET_PATH = os.path.join('../data', 'processed', 'golden_dataset.csv')\n",
    "# Novo batch de dados rotulados que você acabou de criar\n",
    "NEW_LABELS_BATCH_PATH = os.path.join('../data', 'processed', 'batch2_rotulado.csv')\n",
    "# Dataset completo com todas as 1385 empresas, para buscar as features de texto\n",
    "FULL_EXECUTORES_PATH = os.path.join('../data', 'processed', 'df_executores.csv')\n",
    "# Arquivo de saída com o Golden Dataset combinado e atualizado\n",
    "COMBINED_GOLDEN_DATASET_PATH = os.path.join('../data', 'processed', 'golden_dataset_v2.csv')\n",
    "\n",
    "# Mapeamento das colunas do novo batch para o formato padrão\n",
    "MISSION_MAP = {\n",
    "    'M1': 'M1_Agro', 'M2': 'M2_Saude', 'M3': 'M3_Infra_Mobilidade',\n",
    "    'M4': 'M4_Transformacao_Digital', 'M5': 'M5_Bioeconomia_Energia', 'M6': 'M6_Defesa_Soberania'\n",
    "}\n",
    "\n",
    "def process_new_batch(df_new):\n",
    "    \"\"\"\n",
    "    Processa o novo batch de dados, convertendo 'Alinha'/'Não Alinha' para 1/0\n",
    "    e renomeando as colunas.\n",
    "    \"\"\"\n",
    "    # Renomeia a coluna de nome para padronização\n",
    "    df_new = df_new.rename(columns={'Nome da Organização': 'nome_organizacao'})\n",
    "\n",
    "    # Itera sobre as colunas de missão (M1, M2, etc.)\n",
    "    for col in MISSION_MAP.keys():\n",
    "        if col in df_new.columns:\n",
    "            # Converte 'Alinha' para 1 e qualquer outro valor ('Não Alinha') para 0\n",
    "            df_new[col] = df_new[col].apply(lambda x: 1 if str(x).strip() == 'Alinha' else 0)\n",
    "    \n",
    "    # Renomeia as colunas de missão para o formato final (e.g., 'M1' -> 'M1_Agro')\n",
    "    df_new = df_new.rename(columns=MISSION_MAP)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Função principal para integrar os novos rótulos ao Golden Dataset.\n",
    "    \"\"\"\n",
    "    print(\"Iniciando a integração dos novos rótulos...\")\n",
    "\n",
    "    # 1. Carregar os datasets\n",
    "    try:\n",
    "        df_golden_v1 = pd.read_csv(ORIGINAL_GOLDEN_DATASET_PATH)\n",
    "        df_batch2 = pd.read_csv(NEW_LABELS_BATCH_PATH)\n",
    "        df_full = pd.read_csv(FULL_EXECUTORES_PATH)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERRO: Não foi possível encontrar um dos arquivos necessários: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Golden Dataset original com {len(df_golden_v1)} registros.\")\n",
    "    print(f\"Novo batch com {len(df_batch2)} novos rótulos.\")\n",
    "\n",
    "    # 2. Processar o novo batch de dados\n",
    "    df_batch2_processed = process_new_batch(df_batch2)\n",
    "    \n",
    "    # 3. Buscar as features de texto (descrição, etc.) para o novo batch\n",
    "    # Usamos um merge com o dataset completo para trazer as colunas que faltam.\n",
    "    df_batch2_enriched = pd.merge(\n",
    "        df_batch2_processed,\n",
    "        df_full,\n",
    "        on='nome_organizacao',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Garantir que a ordem das colunas seja a mesma do dataset original\n",
    "    df_batch2_enriched = df_batch2_enriched[df_golden_v1.columns]\n",
    "\n",
    "    # 4. Combinar o dataset original com o novo batch enriquecido\n",
    "    df_golden_v2 = pd.concat([df_golden_v1, df_batch2_enriched], ignore_index=True)\n",
    "    \n",
    "    # Remove possíveis duplicatas que possam ter sido adicionadas por engano\n",
    "    df_golden_v2 = df_golden_v2.drop_duplicates(subset=['nome_organizacao'])\n",
    "    \n",
    "    print(f\"O novo Golden Dataset combinado tem {len(df_golden_v2)} registros.\")\n",
    "\n",
    "    # 5. Salvar o novo Golden Dataset\n",
    "    os.makedirs(os.path.dirname(COMBINED_GOLDEN_DATASET_PATH), exist_ok=True)\n",
    "    df_golden_v2.to_csv(COMBINED_GOLDEN_DATASET_PATH, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Integração concluída com sucesso!\")\n",
    "    print(f\"Novo Golden Dataset salvo em: '{COMBINED_GOLDEN_DATASET_PATH}'\")\n",
    "    print(\"\\nPróximo passo recomendado: Execute novamente o script 'train_evaluate_model_v4.py',\")\n",
    "    print(\"ajustando o caminho para carregar este novo arquivo.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a extração de empresas não classificadas...\n",
      "Arquivo de classificação final com 1385 empresas carregado.\n",
      "Foram encontradas 57 empresas que o modelo não conseguiu classificar.\n",
      "--------------------------------------------------\n",
      "Processo concluído com sucesso!\n",
      "O arquivo com as empresas a serem revisadas manualmente foi salvo em: '../data/processed/amostra_nao_classificada.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURAÇÃO ---\n",
    "# Caminho para o arquivo com a classificação final de todas as empresas\n",
    "FINAL_CLASSIFICATION_PATH = os.path.join('../reports', 'classificacao_final_bert.csv')\n",
    "\n",
    "# Caminho para o novo arquivo que conterá apenas as empresas não classificadas\n",
    "UNCLASSIFIED_OUTPUT_PATH = os.path.join('../data', 'processed', 'amostra_nao_classificada.csv')\n",
    "\n",
    "# Nomes das colunas de missão que foram previstas pelo modelo\n",
    "TARGET_LABELS = ['M1_Agro', 'M2_Saude', 'M3_Infra_Mobilidade', 'M4_Transformacao_Digital', 'M5_Bioeconomia_Energia', 'M6_Defesa_Soberania']\n",
    "\n",
    "def extract_unclassified_rows():\n",
    "    \"\"\"\n",
    "    Carrega o dataset classificado, encontra as empresas sem nenhuma missão\n",
    "    atribuída e as salva em um novo arquivo para revisão manual.\n",
    "    \"\"\"\n",
    "    print(\"Iniciando a extração de empresas não classificadas...\")\n",
    "\n",
    "    # 1. Carregar o dataset completo com as previsões\n",
    "    try:\n",
    "        df_final = pd.read_csv(FINAL_CLASSIFICATION_PATH)\n",
    "        print(f\"Arquivo de classificação final com {len(df_final)} empresas carregado.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERRO: Arquivo de classificação final não encontrado em '{FINAL_CLASSIFICATION_PATH}'.\")\n",
    "        print(\"Por favor, execute o script 'predict_full_dataset.py' primeiro.\")\n",
    "        return\n",
    "\n",
    "    # 2. Identificar as linhas não classificadas\n",
    "    # Somamos os valores das colunas de missão para cada linha.\n",
    "    # Se a soma for 0, significa que todas as colunas são 0 (não classificada).\n",
    "    df_unclassified = df_final[df_final[TARGET_LABELS].sum(axis=1) == 0].copy()\n",
    "\n",
    "    if df_unclassified.empty:\n",
    "        print(\"Ótima notícia! Nenhuma empresa não classificada foi encontrada. O modelo atribuiu pelo menos uma missão a todas as empresas.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Foram encontradas {len(df_unclassified)} empresas que o modelo não conseguiu classificar.\")\n",
    "\n",
    "    # 3. Preparar o arquivo para análise manual\n",
    "    # Selecionamos as colunas mais importantes para a sua revisão\n",
    "    output_columns = [\n",
    "        'nome_organizacao', \n",
    "        'descricao_organizacao', \n",
    "        'segmento_atuacao', \n",
    "        'tecnologias_disruptivas'\n",
    "    ]\n",
    "    # Garante que só selecionamos colunas que realmente existem\n",
    "    final_output_columns = [col for col in output_columns if col in df_unclassified.columns]\n",
    "    \n",
    "    df_to_label = df_unclassified[final_output_columns]\n",
    "\n",
    "    # 4. Salvar o resultado\n",
    "    os.makedirs(os.path.dirname(UNCLASSIFIED_OUTPUT_PATH), exist_ok=True)\n",
    "    df_to_label.to_csv(UNCLASSIFIED_OUTPUT_PATH, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"Processo concluído com sucesso!\")\n",
    "    print(f\"O arquivo com as empresas a serem revisadas manualmente foi salvo em: '{UNCLASSIFIED_OUTPUT_PATH}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_unclassified_rows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a integração das revisões manuais...\n",
      "Carregados 1385 registros da classificação automática.\n",
      "Carregados 57 registros da revisão manual.\n",
      "Registros originais foram atualizados com sucesso com os rótulos manuais.\n",
      "--------------------------------------------------\n",
      "Processo de integração concluído!\n",
      "O arquivo final e revisado foi salvo em: '../reports/classificacao_final_revisada.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- CONFIGURAÇÃO ---\n",
    "# Caminho para o arquivo com a classificação completa feita pelo modelo BERT\n",
    "FULL_CLASSIFICATION_PATH = os.path.join('../reports', 'classificacao_final_bert.csv')\n",
    "# Caminho para o seu novo arquivo com os rótulos manuais\n",
    "MANUAL_LABELS_PATH = os.path.join('../data', 'processed', 'amostra_nao_classificada_rotulada.csv')\n",
    "# Caminho para o arquivo de saída final e revisado\n",
    "FINAL_REVISED_OUTPUT_PATH = os.path.join('../reports', 'classificacao_final_revisada.csv')\n",
    "\n",
    "# Mapeamento dos nomes das colunas do seu CSV para os nomes padrão do nosso dataset\n",
    "COLUMN_MAP = {\n",
    "    'Nome da Organização': 'nome_organizacao',\n",
    "    'M1 - Agro': 'M1_Agro',\n",
    "    'M2 - Saúde': 'M2_Saude',\n",
    "    'M3 - Mobilidade': 'M3_Infra_Mobilidade',\n",
    "    'M4 - Digital': 'M4_Transformacao_Digital',\n",
    "    'M5 - Bioeconomia': 'M5_Bioeconomia_Energia',\n",
    "    'M6 - Defesa': 'M6_Defesa_Soberania'\n",
    "}\n",
    "\n",
    "def process_manual_labels(df_manual):\n",
    "    \"\"\"\n",
    "    Processa o DataFrame de rótulos manuais para o formato padrão (1/0).\n",
    "    \"\"\"\n",
    "    # Renomeia as colunas usando o mapa definido\n",
    "    df_manual = df_manual.rename(columns=COLUMN_MAP)\n",
    "\n",
    "    # Converte 'Alinha' para 1 e qualquer outro valor para 0\n",
    "    for col in COLUMN_MAP.values():\n",
    "        if col != 'nome_organizacao' and col in df_manual.columns:\n",
    "            # A regex procura pelo emoji de check ✅\n",
    "            df_manual[col] = df_manual[col].apply(lambda x: 1 if isinstance(x, str) and '✅' in x else 0)\n",
    "    \n",
    "    return df_manual\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Função principal para integrar as revisões manuais ao dataset final.\n",
    "    \"\"\"\n",
    "    print(\"Iniciando a integração das revisões manuais...\")\n",
    "\n",
    "    # 1. Carregar os datasets\n",
    "    try:\n",
    "        df_full_auto = pd.read_csv(FULL_CLASSIFICATION_PATH)\n",
    "        df_manual = pd.read_csv(MANUAL_LABELS_PATH)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERRO: Não foi possível encontrar um dos arquivos necessários: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Carregados {len(df_full_auto)} registros da classificação automática.\")\n",
    "    print(f\"Carregados {len(df_manual)} registros da revisão manual.\")\n",
    "\n",
    "    # 2. Processar o arquivo de rótulos manuais\n",
    "    df_manual_processed = process_manual_labels(df_manual)\n",
    "\n",
    "    # 3. Atualizar o dataset principal com os rótulos manuais\n",
    "    # Usaremos o 'nome_organizacao' como chave para encontrar e atualizar as linhas.\n",
    "    # Definir como índice facilita a atualização.\n",
    "    df_full_auto.set_index('nome_organizacao', inplace=True)\n",
    "    df_manual_processed.set_index('nome_organizacao', inplace=True)\n",
    "\n",
    "    # O método update modifica o DataFrame 'df_full_auto' no local\n",
    "    df_full_auto.update(df_manual_processed)\n",
    "    \n",
    "    # Resetar o índice para que 'nome_organizacao' volte a ser uma coluna\n",
    "    df_full_auto.reset_index(inplace=True)\n",
    "\n",
    "    print(\"Registros originais foram atualizados com sucesso com os rótulos manuais.\")\n",
    "\n",
    "    # 4. Recalcular a coluna de resumo 'missoes_previstas' para refletir as mudanças\n",
    "    # (vamos renomeá-la para 'missoes_finais' para maior clareza)\n",
    "    target_labels = list(COLUMN_MAP.values())[1:] # Pega todos os nomes de missão padrão\n",
    "    \n",
    "    df_full_auto['missoes_finais'] = df_full_auto[target_labels].apply(\n",
    "        lambda row: ', '.join([mission.replace('_', ' ') for mission, assigned in row.items() if assigned]),\n",
    "        axis=1\n",
    "    )\n",
    "    # Remove a coluna de previsões antigas se ela existir\n",
    "    if 'missoes_previstas' in df_full_auto.columns:\n",
    "        df_full_auto = df_full_auto.drop(columns=['missoes_previstas'])\n",
    "\n",
    "    # 5. Salvar o resultado final e revisado\n",
    "    os.makedirs(os.path.dirname(FINAL_REVISED_OUTPUT_PATH), exist_ok=True)\n",
    "    df_full_auto.to_csv(FINAL_REVISED_OUTPUT_PATH, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Processo de integração concluído!\")\n",
    "    print(f\"O arquivo final e revisado foi salvo em: '{FINAL_REVISED_OUTPUT_PATH}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
